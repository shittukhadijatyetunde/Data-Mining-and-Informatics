# Data-Mining-and-Informatics
# Data Mining Project

---

## ğŸ“Œ Project Overview
This repository contains a series of coursework assessments in **Data Science**, where I applied **data preprocessing, visualization, statistical analysis, and machine learning techniques** to real-world datasets. Each task represents a unique data science challenge, showcasing my ability to analyze data, extract insights, and build predictive models.

## ğŸ“ Repository Structure
```
/Data-Science-Coursework
â”‚â”€â”€ /notebooks             # Jupyter notebooks for tasks
â”‚â”€â”€ /reports               # PDF reports summarizing findings
â”‚â”€â”€ /datasets              # Raw and processed datasets
â”‚â”€â”€ /visualizations        # Charts and graphs generated from analysis
â”‚â”€â”€ README.md              # Repository overview and findings
â”‚â”€â”€ LICENSE                # License (if applicable)
â”‚â”€â”€ .gitignore             # Ignoring unnecessary files (e.g., temp, logs)
```

---

## ğŸ” Detailed Findings

### **1ï¸âƒ£ Data Collection & Preprocessing**
- **Files:** `TASK 1.ipynb`
- **Dataset:** Raw datasets requiring preprocessing
- **Methods:** Data cleaning, handling missing values, standardization, encoding
- **Findings:**
  - Identified and handled missing values and duplicated data.
  - Converted categorical variables into numeric format using encoding.
  - Standardized numerical features to improve model performance.
  - Data preprocessing steps significantly improved dataset quality for further analysis.

---

### **2ï¸âƒ£ Exploratory Data Analysis (EDA)**
- **Files:** `TASK 2.ipynb`
- **Dataset:** Various structured datasets
- **Methods:** Data visualization, correlation heatmaps, summary statistics
- **Findings:**
  - Used boxplots and histograms to detect outliers and distribution patterns.
  - Correlation analysis revealed strong relationships between certain variables.
  - Detected anomalies in some features, requiring further preprocessing.
  - Feature engineering opportunities identified for improving predictive models.

---

### **3ï¸âƒ£ Statistical Analysis & Hypothesis Testing**
- **Files:** `TASK 3.ipynb`
- **Dataset:** Experimental datasets for statistical testing
- **Methods:** T-tests, ANOVA, Chi-Square, hypothesis formulation
- **Findings:**
  - T-tests confirmed statistical significance in key comparisons.
  - ANOVA tests helped differentiate group performance within datasets.
  - Chi-Square test indicated dependencies between categorical variables.
  - Statistical tests validated trends observed in exploratory data analysis.

---

### **4ï¸âƒ£ Regression & Predictive Modeling**
- **Files:** `TASK 4.ipynb`
- **Dataset:** Regression-ready datasets
- **Methods:** Linear Regression, Polynomial Regression, Model Evaluation
- **Findings:**
  - **Single Variable Regression:** Predictive power was limited due to weak correlation.
  - **Multiple Regression:** Included multiple features, improving predictive accuracy.
  - **Polynomial Regression:** Outperformed linear regression for non-linear trends.
  - **RÂ² Score:** Improved significantly with more relevant features.
  - Recommended further feature selection and transformation techniques.

---

### **5ï¸âƒ£ Machine Learning Classification**
- **Files:** `TASK 5.ipynb`
- **Dataset:** Classification datasets
- **Methods:** Logistic Regression, Decision Trees, Random Forest, SVM, Model Evaluation
- **Findings:**
  - Decision Trees and Random Forests outperformed Logistic Regression and SVM.
  - Hyperparameter tuning improved accuracy significantly.
  - **Best performing model:** Random Forest with 92% accuracy.
  - Suggested using ensemble methods for even better classification.

---

### **6ï¸âƒ£ Clustering & Unsupervised Learning**
- **Files:** `TASK 6.ipynb`
- **Dataset:** Clustering datasets
- **Methods:** K-Means, Hierarchical Clustering, PCA, Mean Shift
- **Findings:**
  - **K-Means Clustering:** Formed well-defined clusters but required scaling.
  - **Hierarchical Clustering:** Provided better insights into data structure.
  - **PCA:** Reduced dimensionality while retaining variance.
  - **Mean Shift Clustering:** Created distinct clusters, more efficient than K-Means.
  - Final recommendation: Hybrid clustering models for enhanced results.

---

## ğŸ“Œ Key Insights from the Coursework

### **ğŸ  Predictive Modeling & Feature Engineering**
- Feature selection and engineering greatly improved model accuracy.
- External factors significantly influenced predictive outcomes.

### **ğŸŒ Clustering & Data Segmentation**
- Unsupervised learning methods provided meaningful segmentation.
- PCA helped in reducing computational complexity while preserving data structure.

### **ğŸ§  Machine Learning Applications**
- Model selection and hyperparameter tuning were crucial in optimizing accuracy.
- Random Forest emerged as the best classifier among tested models.

---

## ğŸš€ Setup Instructions

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/YOUR-USERNAME/Data-Science-Coursework.git
cd Data-Science-Coursework
```

### **2ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ Run Jupyter Notebooks**
```bash
cd notebooks
jupyter notebook
```

### **4ï¸âƒ£ Execute Scripts**
Open and run any task-specific notebook to reproduce results.

---

## ğŸ¤ Contributors
- **Khadijat Yetunde Shittu**  
- **Collaborators (if any)**

## ğŸ“œ License
This project is licensed under the **MIT License** (if applicable).

## ğŸ“¬ Contact
For inquiries or collaborations, reach out via **[your email]** or **[your LinkedIn profile]**.

